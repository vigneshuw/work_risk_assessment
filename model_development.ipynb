{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os \n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib import feature_extraction as fe\n",
    "from lib import models\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loader\n",
    "\n",
    "- Selecting and loading the required data instances\n",
    "- Loading all data from the LiftingAssessment Task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e157958657110977"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main data path\n",
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "# Selecting Task-2\n",
    "weight_lifting = os.path.join(data_path, \"LiftingAssessment\")\n",
    "# Get all the \".csv\" files\n",
    "all_parsed_files = glob.glob(\"**/*.csv\", root_dir=weight_lifting, recursive=True)\n",
    "\n",
    "# Load the data\n",
    "loaded_data = {}\n",
    "for file_path in all_parsed_files:\n",
    "    # Full path to file\n",
    "    full_path = os.path.join(weight_lifting, file_path)\n",
    "\n",
    "    # Load the time of DAQ\n",
    "    with open(full_path, \"r\") as file_handle:\n",
    "        daq_time = file_handle.readline()\n",
    "        daq_time = daq_time.split(\" \")[-1]\n",
    "        daq_time = int(daq_time[0:-2])\n",
    "    # Read the csv\n",
    "    df = pd.read_csv(full_path, header=\"infer\", skiprows=1)\n",
    "\n",
    "    # Store data\n",
    "    loaded_data[full_path] = {\n",
    "        \"daq_time\": daq_time,\n",
    "        \"df\": df\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "548ed1087dbecda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the counts\n",
    "print(f\"Total number of files loaded - {len(loaded_data.keys())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4991ea2c8b528d58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group by features\n",
    "box_types = [\"Crate\", \"CardboardBox\"]\n",
    "weight_levels = [\"W2\", \"W5\", \"W10\", \"W15\", \"W30\"]\n",
    "labelled_data = {}\n",
    "for box_instance in box_types:\n",
    "    for weight_instance in weight_levels:\n",
    "        labelled_data[box_instance + \"-\" + weight_instance] = []\n",
    "\n",
    "for file_id in loaded_data.keys():\n",
    "    box_instance = file_id.split(os.sep)[-4]\n",
    "    weight_instance = file_id.split(os.sep)[-3]\n",
    "    labelled_data[box_instance + \"-\" + weight_instance].append(file_id)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "987f91d9809a390e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print number of items within each group\n",
    "for class_instance in labelled_data.keys():\n",
    "    print(f\"For the class - {class_instance}, total number of items are {len(labelled_data[class_instance])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "706850e404d09433"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentinels_samplingRate = {\"DAQSentinel01\": [],\n",
    "                          \"DAQSentinel02\": [],\n",
    "                          \"DAQSentinel03\": []}\n",
    "sampling_rates = {}\n",
    "for file_path, data in loaded_data.items():\n",
    "    # Choose the right sentinel\n",
    "    sentinel = file_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "\n",
    "    # Determine sampling rate\n",
    "    total_time = data[\"daq_time\"]\n",
    "    samples = data[\"df\"].shape[0]\n",
    "    sentinels_samplingRate[sentinel].append(samples / total_time)\n",
    "\n",
    "for sentinel in sentinels_samplingRate.keys():\n",
    "    print(\"Sampling Rate for \" + sentinel + \" with mean \" + str(round(np.mean(sentinels_samplingRate[sentinel]), 2)) +\n",
    "          \" and std of \" + str(round(np.std(sentinels_samplingRate[sentinel]), 2)))\n",
    "\n",
    "    # Get the mean sampling rate\n",
    "    sampling_rates[sentinel] = round(np.mean(sentinels_samplingRate[sentinel]), 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e93041dd51c0727e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_combined_dfs = {}\n",
    "sentinels = [\"DAQSentinel01\", \"DAQSentinel02\", \"DAQSentinel03\"]\n",
    "\n",
    "# Group dataframes together\n",
    "for class_instance in labelled_data.keys():\n",
    "    # Differentiate by Sentinels\n",
    "    class_combined_dfs[class_instance] = {}\n",
    "    \n",
    "    # Sentinels data instance counters\n",
    "    counters = {}\n",
    "    \n",
    "    # Go through each file\n",
    "    for file_id in labelled_data[class_instance]:\n",
    "        # Get the sentinel name\n",
    "        sentinel = file_id.split(os.sep)[-1].split(\"_\")[0]\n",
    "        \n",
    "        # Get the dataframe\n",
    "        df = loaded_data[file_id][\"df\"].copy(deep=True)\n",
    "        # Remove the starting and ending data instances\n",
    "        df = df.iloc[int(4 * sampling_rates[sentinel]):int(df.shape[0] - (4 * sampling_rates[sentinel]))]\n",
    "        \n",
    "        if sentinel in list(class_combined_dfs[class_instance].keys()):\n",
    "            class_combined_dfs[class_instance][sentinel] = pd.concat([class_combined_dfs[class_instance][sentinel], df], ignore_index=True, copy=True)\n",
    "            counters[sentinel] += 1\n",
    "        else:\n",
    "            class_combined_dfs[class_instance][sentinel] = df\n",
    "            counters[sentinel] = 1\n",
    "            \n",
    "    # Assert at the end of every class\n",
    "    for s in counters.values():\n",
    "        assert s == 20, \"Each sentinel should add upto 20 counts for two individuals\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ad99416766bd0fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Segmentation\n",
    "\n",
    "- 1 second segments with 250ms overlap between segments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f17f2474da1dfadb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To ensure order\n",
    "data_cols_considered = [\"acc-X\", \"acc-Y\", \"acc-Z\", \"gyr-X\", \"gyr-Y\", \"gyr-Z\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c986812ec6404f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def segment_data(data_array: np.array, segment_window: float, overlap: float, sampling_rate: float):\n",
    "    \n",
    "    window_size = int(segment_window * sampling_rate)\n",
    "    starting_points = np.arange(0, data_array.shape[0], int(window_size * (1 - overlap))).astype(\"uint32\")\n",
    "    \n",
    "    data_segments = list()\n",
    "    for starting_index in starting_points:\n",
    "        if(starting_index + window_size) < data_array.shape[0]:\n",
    "            data_segments.append(\n",
    "                data_array[starting_index:starting_index + window_size, ...])\n",
    "            \n",
    "    return np.array(data_segments)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77936a6bb3946ae7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Segment the data\n",
    "sentinel_segmented_data = {}\n",
    "for class_instance in class_combined_dfs.keys():\n",
    "    sentinel_segmented_data[class_instance] = {}\n",
    "    for sentinel in class_combined_dfs[class_instance].keys():\n",
    "        sentinel_segmented_data[class_instance][sentinel] = segment_data(class_combined_dfs[class_instance][sentinel][data_cols_considered].to_numpy(), 1.0, 0.75, sampling_rates[sentinel])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92ea59ad03c45a45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Extraction key features from the sensor data\n",
    "\n",
    "<span style='color:red'> Note: Run only one cell in the feature extraction segment </span>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d4852cb67bc3460"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time domain Features\n",
    "\n",
    "Only the features from time-domain are extracted\n",
    "\n",
    "1. Time Domain - 11\n",
    "    - RMS\n",
    "    - Variance\n",
    "    - Peak Value\n",
    "    - Crest Factor\n",
    "    - Kurtosis Factor\n",
    "    - Clearance Factor\n",
    "    - Impulse Factor\n",
    "    - Shape Factor\n",
    "    - Line Integral\n",
    "    - Peak to Peak\n",
    "    - Skewness\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcd7f8779413c4d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_extracted_data = {}\n",
    "for class_instance in sentinel_segmented_data.keys():\n",
    "    features_extracted_data[class_instance] = {}\n",
    "    for sentinel in sentinel_segmented_data[class_instance].keys():\n",
    "        data = sentinel_segmented_data[class_instance][sentinel]\n",
    "        \n",
    "        # Apply transformation to every data row\n",
    "        for index, row in enumerate(data):\n",
    "            computed_segments_sensors = []\n",
    "            for i in range(data.shape[-1]):\n",
    "                # apply the transformation\n",
    "                computed_segments_sensors += fe.compute_time_domain_features(row[:, i])\n",
    "            \n",
    "            data_array = np.array(computed_segments_sensors).T\n",
    "            if index == 0:\n",
    "                features_extracted_data[class_instance][sentinel] = copy.deepcopy(data_array[np.newaxis, ...])\n",
    "            else:\n",
    "                features_extracted_data[class_instance][sentinel] = np.append(features_extracted_data[class_instance][sentinel], copy.deepcopy(data_array[np.newaxis, ...]), axis=0)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9116d6529f14415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Domain + Frequency Domain Features\n",
    "\n",
    "The features here include a combination of time domain and frequency domain features\n",
    "\n",
    "1. Time domain features - 11 (Same as above)\n",
    "2. Frequency domain features - 3\n",
    "    - Peak FFT\n",
    "    - Energy FFT\n",
    "    - Power Spectral Density of FFT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d38713251425180"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_extracted_data = {}\n",
    "for class_instance in sentinel_segmented_data.keys():\n",
    "    features_extracted_data[class_instance] = {}\n",
    "    for sentinel in sentinel_segmented_data[class_instance].keys():\n",
    "        data = sentinel_segmented_data[class_instance][sentinel]\n",
    "        \n",
    "        # Select arguments based on sentinel\n",
    "        freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": 200, \"noverlap\": 100, \"fs\": sampling_rates[sentinel]}]\n",
    "        \n",
    "        # Apply transformation to every data row\n",
    "        for index, row in enumerate(data):\n",
    "            computed_segments_sensors = []\n",
    "            for i in range(data.shape[-1]):\n",
    "                # apply the transformation\n",
    "                computed_segments_sensors += fe.compute_time_and_frequency_features(row[:, i], freq_args=freq_args)\n",
    "            \n",
    "            data_array = np.array(computed_segments_sensors).T\n",
    "            if index == 0:\n",
    "                features_extracted_data[class_instance][sentinel] = copy.deepcopy(data_array[np.newaxis, ...])\n",
    "            else:\n",
    "                features_extracted_data[class_instance][sentinel] = np.append(features_extracted_data[class_instance][sentinel], copy.deepcopy(data_array[np.newaxis, ...]), axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10d19ad7afa371f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frequency Domain + Time-Frequency Domain Features\n",
    "\n",
    "Includes a total of 6 features\n",
    "\n",
    "1. Frequency Domain - 3\n",
    "2. Time-Frequency Domain - 6\n",
    "    - Energy WPD (Wavelet Packet Decomposition) 1st Order\n",
    "    - Energy WPD 2nd Order\n",
    "    - Energy WPD 3rd Order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00922b499d97143"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_extracted_data = {}\n",
    "for class_instance in sentinel_segmented_data.keys():\n",
    "    features_extracted_data[class_instance] = {}\n",
    "    for sentinel in sentinel_segmented_data[class_instance].keys():\n",
    "        data = sentinel_segmented_data[class_instance][sentinel]\n",
    "        \n",
    "        # Select arguments based on sentinel\n",
    "        freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": 200, \"noverlap\": 100, \"fs\": sampling_rates[sentinel]}]\n",
    "        freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "        \n",
    "        # Apply transformation to every data row\n",
    "        for index, row in enumerate(data):\n",
    "            computed_segments_sensors = []\n",
    "            for i in range(data.shape[-1]):\n",
    "                # apply the transformation\n",
    "                computed_segments_sensors += fe.compute_frequency_and_time_frequency_features(row[:, i], freq_args=freq_args, freq_time_args=freq_time_args)\n",
    "            \n",
    "            data_array = np.array(computed_segments_sensors).T\n",
    "            if index == 0:\n",
    "                features_extracted_data[class_instance][sentinel] = copy.deepcopy(data_array[np.newaxis, ...])\n",
    "            else:\n",
    "                features_extracted_data[class_instance][sentinel] = np.append(features_extracted_data[class_instance][sentinel], copy.deepcopy(data_array[np.newaxis, ...]), axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "586074184d2bcfda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Domain + Frequency Domain + Time-frequency Domain Features\n",
    "\n",
    "Extracting the following features, a total of 17 features\n",
    "\n",
    "1. Time Domain Features - 11\n",
    "2. Frequency Domain Features - 3\n",
    "3. Time-frequency Domain Features - 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e0df0cfda2c4426"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_extracted_data = {}\n",
    "for class_instance in sentinel_segmented_data.keys():\n",
    "    features_extracted_data[class_instance] = {}\n",
    "    for sentinel in sentinel_segmented_data[class_instance].keys():\n",
    "        data = sentinel_segmented_data[class_instance][sentinel]\n",
    "        \n",
    "        # Select arguments based on sentinel\n",
    "        freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": 200, \"noverlap\": 100, \"fs\": sampling_rates[sentinel]}]\n",
    "        freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "        \n",
    "        # Apply transformation to every data row\n",
    "        for index, row in enumerate(data):\n",
    "            computed_segments_sensors = []\n",
    "            for i in range(data.shape[-1]):\n",
    "                # apply the transformation\n",
    "                computed_segments_sensors += fe.compute_all_features(row[:, i], freq_args=freq_args, freq_time_args=freq_time_args)\n",
    "            \n",
    "            data_array = np.array(computed_segments_sensors).T\n",
    "            if index == 0:\n",
    "                features_extracted_data[class_instance][sentinel] = copy.deepcopy(data_array[np.newaxis, ...])\n",
    "            else:\n",
    "                features_extracted_data[class_instance][sentinel] = np.append(features_extracted_data[class_instance][sentinel], copy.deepcopy(data_array[np.newaxis, ...]), axis=0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1b3009c41949794"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Development\n",
    "\n",
    "- Choose among the 10 available models\n",
    "- set the parameters appropriately\n",
    "- Train the model and get the metrics\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f08b725a7da7832"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tuned hyperparameters\n",
    "model_params = {\n",
    "    \"LogisticRegression\" : {\"class_weight\": \"balanced\", \"max_iter\": 5000, \"n_jobs\": 4, \"tol\": 0.0001},\n",
    "    \"DecisionTreeClassifier\": {'class_weight': 'balanced', 'max_depth': 50, 'min_samples_leaf': 20, 'min_samples_split': 20},\n",
    "    \"KNeighborsClassifier\": {'n_neighbors': 10, 'weights': 'uniform'},\n",
    "    \"SVC\": {'class_weight': 'balanced', 'kernel': 'poly', 'tol': 1e-07},\n",
    "    \"BaggingClassifier\": {\"n_estimators\": 100},\n",
    "    \"RandomForestClassifier\": {'class_weight': 'balanced', 'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 100, 'n_estimators': 100},\n",
    "    # \"GradientBoostingClassifier\": {\"loss\": \"log_loss\", \"learning_rate\": 0.001, \"n_estimators\": 100, \"min_samples_split\": 50, \"min_samples_leaf\": 5, \"max_depth\": 5, \"verbose\": 0, \"tol\":1e-7},\n",
    "    \"AdaBoostClassifier\": {\"base_estimator\": BaggingClassifier(n_estimators=100),\"n_estimators\": 500, \"learning_rate\": 0.001},\n",
    "    \"MLPClassifier\": {\"hidden_layer_sizes\": (50, 20), \"max_iter\": 200}\n",
    "}\n",
    "\n",
    "# NIOSH labels\n",
    "labels = {\n",
    "    \"Crate-W2\": 0,\n",
    "    \"Crate-W5\": 0,\n",
    "    \"Crate-W10\": 0,\n",
    "    \"Crate-W15\": 1,\n",
    "    \"Crate-W30\": 1,\n",
    "    \"CardboardBox-W2\": 0,\n",
    "    \"CardboardBox-W5\": 0,\n",
    "    \"CardboardBox-W10\": 0,\n",
    "    \"CardboardBox-W15\": 1,\n",
    "    \"CardboardBox-W30\": 1,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a63f55f7258bd004"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Individual Sentinels\n",
    "\n",
    "Model development by considering one Sentinel at a time\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f15a4bcfc74055c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose the sentinel for model training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "813f463ae2a75c59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentinel = \"DAQSentinel02\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a83a59efcf3859e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct training data and labels\n",
    "for index, class_instance in enumerate(features_extracted_data.keys()):\n",
    "    # Select sentinel\n",
    "    if index == 0:\n",
    "        X_train = features_extracted_data[class_instance][sentinel]\n",
    "        y_train = np.array([labels[class_instance]] * features_extracted_data[class_instance][sentinel].shape[0])[:, np.newaxis]\n",
    "    else:\n",
    "        X_train = np.append(X_train, features_extracted_data[class_instance][sentinel], axis=0)\n",
    "        y_train = np.append(y_train, np.array([labels[class_instance]] * features_extracted_data[class_instance][sentinel].shape[0])[:, np.newaxis], axis=0)\n",
    "        \n",
    "# Print results\n",
    "print(f\"Shape of X-train is {X_train.shape}\")\n",
    "y_train = y_train.squeeze(axis=-1)\n",
    "print(f\"Shape of y-train is {y_train.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a30e930524114f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create models repo\n",
    "models_repo = models.Models()\n",
    "# Initialize\n",
    "models_repo.create_models(model_params)\n",
    "\n",
    "# 10-fold CV\n",
    "cv_results_summary = models_repo.train_models_cvfolds(X_train, y_train, kfolds=10, summarize_results=True, standardize=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a770efcf0214ad3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model names\n",
    "model_association = [\n",
    "    \"LogisticRegression\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"SVC\",\n",
    "    \"BaggingClassifier\",\n",
    "    \"RandomForestClassifier\", \n",
    "    \"AdaBoostClassifier\",\n",
    "    \"MLPClassifier\"\n",
    "]\n",
    "\n",
    "# Make a copy\n",
    "temp = copy.deepcopy(cv_results_summary)\n",
    "\n",
    "for index, model_name in enumerate(model_association):\n",
    "\n",
    "    temp[model_name].columns = pd.MultiIndex.from_product([[model_name], temp[model_name].columns])\n",
    "    # Append columns\n",
    "    if index == 0:\n",
    "        combined_cv_results = temp[model_name]\n",
    "    else:\n",
    "        combined_cv_results = pd.concat([combined_cv_results, temp[model_name]], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6c956ace46df8d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_cv_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd890a24c7c1dddd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All Sentinels\n",
    "\n",
    "- Considering all Sentinels in the model training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c5ef03b2050d97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct training data and labels\n",
    "for index, class_instance in enumerate(features_extracted_data.keys()):\n",
    "    \n",
    "    # Find the sentinel with min samples\n",
    "    samples = []\n",
    "    for sentinel in sentinels:\n",
    "        samples.append(features_extracted_data[class_instance][sentinel].shape[0])\n",
    "    \n",
    "    min_samples = min(samples)\n",
    "\n",
    "    for index2, sentinel in enumerate(sentinels):\n",
    "        if index2 == 0:\n",
    "            sub_X_train = features_extracted_data[class_instance][sentinel][0:min_samples, ...]\n",
    "        else:\n",
    "            sub_X_train = np.concatenate((sub_X_train, features_extracted_data[class_instance][sentinel][0:min_samples, ...]), axis=-1)\n",
    "    \n",
    "    if index == 0:\n",
    "        X_train = copy.deepcopy(sub_X_train)\n",
    "        y_train = np.array([labels[class_instance]] * sub_X_train.shape[0])[:, np.newaxis]\n",
    "    else:\n",
    "        X_train = np.append(X_train, copy.deepcopy(sub_X_train), axis=0)\n",
    "        y_train = np.append(y_train, np.array([labels[class_instance]] * sub_X_train.shape[0])[:, np.newaxis], axis=0)\n",
    "        \n",
    "# Print results\n",
    "print(f\"Shape of X-train is {X_train.shape}\")\n",
    "y_train = y_train.squeeze(axis=-1)\n",
    "print(f\"Shape of y-train is {y_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc183fa10caf3911"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create models repo\n",
    "models_repo = models.Models()\n",
    "# Initialize\n",
    "models_repo.create_models(model_params)\n",
    "\n",
    "# 10-fold CV\n",
    "cv_results_summary = models_repo.train_models_cvfolds(X_train, y_train, kfolds=10, summarize_results=True, standardize=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef1eccc9d31a0cfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model names\n",
    "model_association = [\n",
    "    \"LogisticRegression\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"SVC\",\n",
    "    \"BaggingClassifier\",\n",
    "    \"RandomForestClassifier\", \n",
    "    \"AdaBoostClassifier\",\n",
    "    \"MLPClassifier\"\n",
    "]\n",
    "\n",
    "# Make a copy\n",
    "temp = copy.deepcopy(cv_results_summary)\n",
    "\n",
    "for index, model_name in enumerate(model_association):\n",
    "\n",
    "    temp[model_name].columns = pd.MultiIndex.from_product([[model_name], temp[model_name].columns])\n",
    "    # Append columns\n",
    "    if index == 0:\n",
    "        combined_cv_results = temp[model_name]\n",
    "    else:\n",
    "        combined_cv_results = pd.concat([combined_cv_results, temp[model_name]], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afce8078219cbd26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_cv_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c6f8e677f01c9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9641a503dfb72fe8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e0551f8d19726d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"LogisticRegression\" : {\"class_weight\": \"balanced\", \"max_iter\": 5000, \"n_jobs\": 4},\n",
    "    \"DecisionTreeClassifier\": {\"min_samples_split\": 20},\n",
    "    \"KNeighborsClassifier\": {\"n_neighbors\": 10},\n",
    "    \"SVC\": {\"kernel\": \"rbf\", \"tol\":1e-7},\n",
    "    \"BaggingClassifier\": {\"n_estimators\": 50},\n",
    "    \"RandomForestClassifier\": {\"n_estimators\": 100, \"min_samples_split\": 100, \"class_weight\": \"balanced\"},\n",
    "    # \"GradientBoostingClassifier\": {\"loss\": \"log_loss\", \"learning_rate\": 0.001, \"n_estimators\": 100, \"min_samples_split\": 50, \"min_samples_leaf\": 5, \"max_depth\": 5, \"verbose\": 0, \"tol\":1e-7},\n",
    "    \"AdaBoostClassifier\": {\"n_estimators\": 100, \"learning_rate\": 0.0001},\n",
    "    \"MLPClassifier\": {\"hidden_layer_sizes\": (100, 50), \"max_iter\": 500}\n",
    "}\n",
    "\n",
    "# Prospective hyperparameters\n",
    "hp = {\n",
    "    \"LogisticRegression\" : {\"tol\": [0.0001, 0.00005, 0.0000005], \"max_iter\": [5000, 10000, 20000], \"multi_class\": [\"multinomial\"], \"n_jobs\": [4], \"class_weight\": [\"balanced\"]},\n",
    "    \"DecisionTreeClassifier\": {\"min_samples_split\": [20, 50, 100], \"max_depth\": [None, 5, 10, 15, 50], \"min_samples_leaf\":[1, 20, 100, 200], \"class_weight\": [\"balanced\"]},\n",
    "    \"KNeighborsClassifier\": {\"n_neighbors\": [10, 5, 20, 50, 100], \"weights\":[\"uniform\", \"distance\"]},\n",
    "    \"SVC\": {\"kernel\": [\"linear\", \"poly\", \"rbf\"], \"tol\":[1e-7, 1e-3], \"class_weight\": [\"balanced\"]},\n",
    "    \"BaggingClassifier\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"RandomForestClassifier\": {\"n_estimators\": [100, 50, 200], \"min_samples_split\": [100, 500], \"max_depth\": [None, 5, 10, 15, 50], \"min_samples_leaf\":[1, 100, 500], \"class_weight\": [\"balanced\"]},\n",
    "    # \"GradientBoostingClassifier\": {\"loss\": [\"log_loss\"], \"learning_rate\": [0.1, 0.001, 0.0001], \"n_estimators\": [50, 100, 400], \"min_samples_split\": [10, 20, 50, 100], \"min_samples_leaf\": [1, 10, 50], \"max_depth\": [None, 5, 10, 15, 50], \"tol\":[1e-7, 1e-3]},\n",
    "    \"AdaBoostClassifier\": {\"n_estimators\": [20, 100, 200, 500], \"learning_rate\": [0.01, 0.001]},\n",
    "    \"MLPClassifier\": {\"hidden_layer_sizes\": [(100, 50), (100, 50, 20), (50, 20)], \"max_iter\": [100, 200, 500]}\n",
    "}\n",
    "\n",
    "# NIOSH labels\n",
    "labels = {\n",
    "    \"Crate-W2\": 0,\n",
    "    \"Crate-W5\": 0,\n",
    "    \"Crate-W10\": 0,\n",
    "    \"Crate-W15\": 1,\n",
    "    \"Crate-W30\": 1,\n",
    "    \"CardboardBox-W2\": 0,\n",
    "    \"CardboardBox-W5\": 0,\n",
    "    \"CardboardBox-W10\": 0,\n",
    "    \"CardboardBox-W15\": 1,\n",
    "    \"CardboardBox-W30\": 1,\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d37745d35f1fa969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct training data and labels\n",
    "for index, class_instance in enumerate(features_extracted_data.keys()):\n",
    "    \n",
    "    # Find the sentinel with min samples\n",
    "    samples = []\n",
    "    for sentinel in sentinels:\n",
    "        samples.append(features_extracted_data[class_instance][sentinel].shape[0])\n",
    "    \n",
    "    min_samples = min(samples)\n",
    "\n",
    "    for index2, sentinel in enumerate(sentinels):\n",
    "        if index2 == 0:\n",
    "            sub_X_train = features_extracted_data[class_instance][sentinel][0:min_samples, ...]\n",
    "        else:\n",
    "            sub_X_train = np.concatenate((sub_X_train, features_extracted_data[class_instance][sentinel][0:min_samples, ...]), axis=-1)\n",
    "    \n",
    "    if index == 0:\n",
    "        X_train = copy.deepcopy(sub_X_train)\n",
    "        y_train = np.array([labels[class_instance]] * sub_X_train.shape[0])[:, np.newaxis]\n",
    "    else:\n",
    "        X_train = np.append(X_train, copy.deepcopy(sub_X_train), axis=0)\n",
    "        y_train = np.append(y_train, np.array([labels[class_instance]] * sub_X_train.shape[0])[:, np.newaxis], axis=0)\n",
    "        \n",
    "# Print results\n",
    "print(f\"Shape of X-train is {X_train.shape}\")\n",
    "y_train = y_train.squeeze(axis=-1)\n",
    "print(f\"Shape of y-train is {y_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8fe1acd1a737197"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create repo of models for hyperparameter optimization\n",
    "models_repo_hyperopt = models.Models()\n",
    "# Initialize the models\n",
    "models_repo_hyperopt.create_models(model_params)\n",
    "\n",
    "# Optimize the hyperparameters for all models\n",
    "models_repo_hyperopt.optimize_hyperparameters(hyperparameters=hp, X_train=X_train, y_train=y_train, standardize=True)\n",
    "\n",
    "# Print the optimized f1-scores\n",
    "print(\"F1-Scores\")\n",
    "for model_name in models_repo_hyperopt.hyper_opt_model_scores.keys():\n",
    "\n",
    "    print(f\"{model_name} - {models_repo_hyperopt.hyper_opt_model_scores[model_name]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3818a2b9cb28d31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_repo_hyperopt.hyper_opt_model_params"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7193a4592eef7ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "508fd1e7f8d9ec3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
