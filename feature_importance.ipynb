{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from lib import feature_extraction as fe, models\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import  numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "\n",
    "font = {\n",
    "    \"family\": \"sans-serif\",\n",
    "    \"weight\": \"bold\",\n",
    "    \"size\": 12\n",
    "}\n",
    "matplotlib.rc(\"font\", **font)\n",
    "\n",
    "# Figures save paths\n",
    "with open(\"secrets.yml\", \"r\") as file_handle:\n",
    "    fig_save_paths = yaml.load(file_handle, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loader\n",
    "\n",
    "- Selecting and loading the required data instances\n",
    "- Loading all data from the LiftingAssessment Task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec45303e83cdf530"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main data path\n",
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "# Selecting Task-2\n",
    "weight_lifting = os.path.join(data_path, \"LiftingAssessment\")\n",
    "# Get all the \".csv\" files\n",
    "all_parsed_files = glob.glob(\"**/*.csv\", root_dir=weight_lifting, recursive=True)\n",
    "\n",
    "# Load the data\n",
    "loaded_data = {}\n",
    "for file_path in all_parsed_files:\n",
    "    # Full path to file\n",
    "    full_path = os.path.join(weight_lifting, file_path)\n",
    "\n",
    "    # Load the time of DAQ\n",
    "    with open(full_path, \"r\") as file_handle:\n",
    "        daq_time = file_handle.readline()\n",
    "        daq_time = daq_time.split(\" \")[-1]\n",
    "        daq_time = int(daq_time[0:-2])\n",
    "    # Read the csv\n",
    "    df = pd.read_csv(full_path, header=\"infer\", skiprows=1)\n",
    "\n",
    "    # Store data\n",
    "    loaded_data[full_path] = {\n",
    "        \"daq_time\": daq_time,\n",
    "        \"df\": df\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35de21df50d1ee96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the counts\n",
    "print(f\"Total number of files loaded - {len(loaded_data.keys())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64aa4d4aa3776a9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group by features\n",
    "box_types = [\"Crate\", \"CardboardBox\"]\n",
    "weight_levels = [\"W2\", \"W5\", \"W10\", \"W15\", \"W20\", \"W30\"]\n",
    "labelled_data = {}\n",
    "for box_instance in box_types:\n",
    "    for weight_instance in weight_levels:\n",
    "        labelled_data[box_instance + \"-\" + weight_instance] = []\n",
    "\n",
    "for file_id in loaded_data.keys():\n",
    "    box_instance = file_id.split(os.sep)[-4]\n",
    "    weight_instance = file_id.split(os.sep)[-3]\n",
    "    labelled_data[box_instance + \"-\" + weight_instance].append(file_id)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7706c393da788a3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print number of items within each group\n",
    "for class_instance in labelled_data.keys():\n",
    "    print(f\"For the class - {class_instance}, total number of items are {len(labelled_data[class_instance])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f471c73449c0f70c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentinels_samplingRate = {\"DAQSentinel01\": [],\n",
    "                          \"DAQSentinel02\": [],\n",
    "                          \"DAQSentinel03\": []}\n",
    "sampling_rates = {}\n",
    "for file_path, data in loaded_data.items():\n",
    "    # Choose the right sentinel\n",
    "    sentinel = file_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "\n",
    "    # Determine sampling rate\n",
    "    total_time = data[\"daq_time\"]\n",
    "    samples = data[\"df\"].shape[0]\n",
    "    sentinels_samplingRate[sentinel].append(samples / total_time)\n",
    "\n",
    "for sentinel in sentinels_samplingRate.keys():\n",
    "    print(\"Sampling Rate for \" + sentinel + \" with mean \" + str(round(np.mean(sentinels_samplingRate[sentinel]), 2)) +\n",
    "          \" and std of \" + str(round(np.std(sentinels_samplingRate[sentinel]), 2)))\n",
    "\n",
    "    # Get the mean sampling rate\n",
    "    sampling_rates[sentinel] = round(np.mean(sentinels_samplingRate[sentinel]), 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e50ac0550332ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_combined_dfs = {}\n",
    "num_individuals_analyzed  = 3\n",
    "sentinels = [\"DAQSentinel01\", \"DAQSentinel02\", \"DAQSentinel03\"]\n",
    "\n",
    "# Group dataframes together\n",
    "for class_instance in labelled_data.keys():\n",
    "    # Differentiate by Sentinels\n",
    "    class_combined_dfs[class_instance] = {}\n",
    "    \n",
    "    # Sentinels data instance counters\n",
    "    counters = {}\n",
    "    \n",
    "    # Go through each file\n",
    "    for file_id in labelled_data[class_instance]:\n",
    "        # Get the sentinel name\n",
    "        sentinel = file_id.split(os.sep)[-1].split(\"_\")[0]\n",
    "        \n",
    "        # Get the dataframe\n",
    "        df = loaded_data[file_id][\"df\"].copy(deep=True)\n",
    "        # Remove the starting and ending data instances\n",
    "        df = df.iloc[int(4 * sampling_rates[sentinel]):int(df.shape[0] - (4 * sampling_rates[sentinel]))]\n",
    "        \n",
    "        if sentinel in list(class_combined_dfs[class_instance].keys()):\n",
    "            class_combined_dfs[class_instance][sentinel] = pd.concat([class_combined_dfs[class_instance][sentinel], df], ignore_index=True, copy=True)\n",
    "            counters[sentinel] += 1\n",
    "        else:\n",
    "            class_combined_dfs[class_instance][sentinel] = df\n",
    "            counters[sentinel] = 1\n",
    "            \n",
    "    # Assert at the end of every class\n",
    "    for s in counters.values():\n",
    "        assert s == 10 * num_individuals_analyzed, \"Each sentinel should add upto 20 counts for two individuals\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df6f32373f0b8a1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Segmentation\n",
    "\n",
    "- 1 second segments with 250ms overlap between segments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9224bcdf52fd3467"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To ensure order\n",
    "data_cols_considered = [\"acc-X\", \"acc-Y\", \"acc-Z\", \"gyr-X\", \"gyr-Y\", \"gyr-Z\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8854977032ab6ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def segment_data(data_array: np.array, segment_window: float, overlap: float, sampling_rate: float):\n",
    "    \n",
    "    window_size = int(segment_window * sampling_rate)\n",
    "    starting_points = np.arange(0, data_array.shape[0], int(window_size * (1 - overlap))).astype(\"uint32\")\n",
    "    \n",
    "    data_segments = list()\n",
    "    for starting_index in starting_points:\n",
    "        if(starting_index + window_size) < data_array.shape[0]:\n",
    "            data_segments.append(\n",
    "                data_array[starting_index:starting_index + window_size, ...])\n",
    "            \n",
    "    return np.array(data_segments)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b96acaf466dd70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Segment the data\n",
    "sentinel_segmented_data = {}\n",
    "for class_instance in class_combined_dfs.keys():\n",
    "    sentinel_segmented_data[class_instance] = {}\n",
    "    for sentinel in class_combined_dfs[class_instance].keys():\n",
    "        sentinel_segmented_data[class_instance][sentinel] = segment_data(class_combined_dfs[class_instance][sentinel][data_cols_considered].to_numpy(), 1.0, 0.75, sampling_rates[sentinel])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58a69c1ef8b24f97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Extraction key features from the sensor data. Consider time domain, frequency domain, and time-frequency domain features\n",
    "\n",
    "Extracting the following features, a total of 17 features\n",
    "\n",
    "1. Time Domain Features - 11\n",
    "2. Frequency Domain Features - 3\n",
    "3. Time-frequency Domain Features - 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2998c78af04cc7ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_extracted_data = {}\n",
    "for class_instance in sentinel_segmented_data.keys():\n",
    "    features_extracted_data[class_instance] = {}\n",
    "    for sentinel in sentinel_segmented_data[class_instance].keys():\n",
    "        data = sentinel_segmented_data[class_instance][sentinel]\n",
    "        \n",
    "        # Select arguments based on sentinel\n",
    "        freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": 200, \"noverlap\": 100, \"fs\": sampling_rates[sentinel]}]\n",
    "        freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "        \n",
    "        # Apply transformation to every data row\n",
    "        for index, row in enumerate(data):\n",
    "            computed_segments_sensors = []\n",
    "            for i in range(data.shape[-1]):\n",
    "                # apply the transformation\n",
    "                computed_segments_sensors += fe.compute_all_features(row[:, i], freq_args=freq_args, freq_time_args=freq_time_args)\n",
    "            \n",
    "            data_array = np.array(computed_segments_sensors).T\n",
    "            if index == 0:\n",
    "                features_extracted_data[class_instance][sentinel] = copy.deepcopy(data_array[np.newaxis, ...])\n",
    "            else:\n",
    "                features_extracted_data[class_instance][sentinel] = np.append(features_extracted_data[class_instance][sentinel], copy.deepcopy(data_array[np.newaxis, ...]), axis=0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55c6e5d9dde54f99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Training Data\n",
    "\n",
    "- Training, and Testing data generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66c1a95b006ebcb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NIOSH labels\n",
    "labels = {\n",
    "    \"Crate-W2\": 0,\n",
    "    \"Crate-W5\": 0,\n",
    "    \"Crate-W10\": 0,\n",
    "    \"Crate-W15\": 0,\n",
    "    \"Crate-W20\": 1,\n",
    "    \"Crate-W30\": 1,\n",
    "    \"CardboardBox-W2\": 0,\n",
    "    \"CardboardBox-W5\": 0,\n",
    "    \"CardboardBox-W10\": 0,\n",
    "    \"CardboardBox-W15\": 0,\n",
    "    \"CardboardBox-W20\": 1,\n",
    "    \"CardboardBox-W30\": 1,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea9d9aeeefbab154"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct training data and labels\n",
    "for index, class_instance in enumerate(features_extracted_data.keys()):\n",
    "    \n",
    "    # Find the sentinel with min samples\n",
    "    samples = []\n",
    "    for sentinel in sentinels:\n",
    "        samples.append(features_extracted_data[class_instance][sentinel].shape[0])\n",
    "    \n",
    "    min_samples = min(samples)\n",
    "\n",
    "    for index2, sentinel in enumerate(sentinels):\n",
    "        if index2 == 0:\n",
    "            sub_X_train = features_extracted_data[class_instance][sentinel][0:min_samples, ...]\n",
    "        else:\n",
    "            sub_X_train = np.concatenate((sub_X_train, features_extracted_data[class_instance][sentinel][0:min_samples, ...]), axis=-1)\n",
    "    \n",
    "    if index == 0:\n",
    "        X = copy.deepcopy(sub_X_train)\n",
    "        y = np.array([labels[class_instance]] * sub_X_train.shape[0])[:, np.newaxis]\n",
    "    else:\n",
    "        X = np.append(X, copy.deepcopy(sub_X_train), axis=0)\n",
    "        y = np.append(y, np.array([labels[class_instance]] * sub_X_train.shape[0])[:, np.newaxis], axis=0)\n",
    "        \n",
    "# Print results\n",
    "print(f\"Shape of X-train is {X.shape}\")\n",
    "y = y.squeeze(axis=-1)\n",
    "print(f\"Shape of y-train is {y.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e89f463031480eb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split to train and test\n",
    "\n",
    "- Proportion is 80% on the training\n",
    "- Standardize the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a066f592c5884d71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89648f85490ec608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalization if required\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f48de6ea77de1af3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model development\n",
    "\n",
    "- Choose the best performing model from the previous studies\n",
    "- Preferably the one with best performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cce3f3fa596833b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"LogisticRegression\" : {\"class_weight\": \"balanced\", \"max_iter\": 5000, \"n_jobs\": 4, \"tol\": 0.0001},\n",
    "    \"DecisionTreeClassifier\": {'class_weight': 'balanced', 'max_depth': 50, 'min_samples_leaf': 20, 'min_samples_split': 20},\n",
    "    \"KNeighborsClassifier\": {'n_neighbors': 10, 'weights': 'uniform'},\n",
    "    \"SVC\": {'class_weight': 'balanced', 'kernel': 'poly', 'tol': 1e-07},\n",
    "    \"BaggingClassifier\": {\"n_estimators\": 100},\n",
    "    \"RandomForestClassifier\": {'class_weight': 'balanced', 'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 100, 'n_estimators': 100},\n",
    "    # \"GradientBoostingClassifier\": {\"loss\": \"log_loss\", \"learning_rate\": 0.001, \"n_estimators\": 100, \"min_samples_split\": 50, \"min_samples_leaf\": 5, \"max_depth\": 5, \"verbose\": 0, \"tol\":1e-7},\n",
    "    \"AdaBoostClassifier\": {\"base_estimator\": BaggingClassifier(n_estimators=100),\"n_estimators\": 500, \"learning_rate\": 0.001},\n",
    "    \"MLPClassifier\": {\"hidden_layer_sizes\": (50, 20), \"max_iter\": 200}\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0630699b17b92ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create repo of models\n",
    "models_repo = models.Models()\n",
    "# Initialize the models\n",
    "models_repo.create_models(model_params)\n",
    "\n",
    "# Train the models\n",
    "models_repo.train_models(X_train, y_train, verbose=True)\n",
    "\n",
    "# Score the models\n",
    "for model_name in models_repo.trained_model_dict.keys():\n",
    "    print(f\"Score for the model-{model_name} is {models_repo.trained_model_dict[model_name].score(X_test, y_test)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b561dd210be7b5c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the top performing models\n",
    "f1_scores_models = {}\n",
    "for model_name in models_repo.trained_model_dict.keys():\n",
    "    f1_scores_models[model_name] = models_repo.trained_model_dict[model_name].score(X_test, y_test)\n",
    "    \n",
    "# Find the top performing model\n",
    "top_f1score_models = []\n",
    "for _ in range(4):\n",
    "    f1_score = 0\n",
    "    for model_name in f1_scores_models.keys():\n",
    "        if f1_scores_models[model_name] > f1_score and model_name not in top_f1score_models:\n",
    "            top_model = model_name\n",
    "    \n",
    "    top_f1score_models.append(top_model)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd0be7d55c54ec6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The total 4 models are: \", end=\"\")\n",
    "for model_name in top_f1score_models:\n",
    "    print(f\"{model_name}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463258944a7128be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e113100ce7c1e027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Names of each of the extracted features\n",
    "feature_names = [\n",
    "    \"rms\",\n",
    "    \"variance\",\n",
    "    \"peak value\",\n",
    "    \"crest factor\",\n",
    "    \"kurtosis fisher\",\n",
    "    \"clearance factor\",\n",
    "    \"impulse factor\",\n",
    "    \"shape factor\",\n",
    "    \"line integral\",\n",
    "    \"peak to peak\",\n",
    "    \"skewness\",\n",
    "    \"peak fft\",\n",
    "    \"energy fft\",\n",
    "    \"PSD fft\",\n",
    "    \"WPD-1 energy\",\n",
    "    \"WPD-2 energy\",\n",
    "    \"WPD-3 energy\"\n",
    "]\n",
    "\n",
    "combined_features = []\n",
    "for sentinel in sentinels:\n",
    "    for col in data_cols_considered:\n",
    "        for ext_feature_name in feature_names:\n",
    "            combined_features.append(sentinel + \"|\" + col + \"|\" + ext_feature_name)\n",
    "            \n",
    "feature_names = combined_features\n",
    "            \n",
    "print(f\"Total combined features - {len(feature_names)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83cf25697d155ffd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance_results = {}\n",
    "importances_mean = {}\n",
    "importances_std = {}\n",
    "\n",
    "for model_name in models_repo.trained_model_dict.keys():\n",
    "    clf = models_repo.trained_model_dict[model_name]\n",
    "    feature_importance_results[model_name] = permutation_importance(clf, X_test, y_test, n_repeats=500, random_state=42, n_jobs=8)\n",
    "    # Importances mean and std\n",
    "    importances_mean[model_name] = feature_importance_results[model_name].importances_mean\n",
    "    importances_std[model_name] = feature_importance_results[model_name].importances_std\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5245a4b007314ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Top k features\n",
    "\n",
    "Selecting and plotting the top 5 important features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854c7c700510aab4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top = 10\n",
    "\n",
    "# Select the importances mean and std\n",
    "selected_importances_mean = {}\n",
    "selected_importances_std = {}\n",
    "selected_names = {}\n",
    "for model_name in importances_mean.keys():\n",
    "    maxk_indices = np.squeeze(np.argpartition(importances_mean[model_name], -top)[-top:])\n",
    "    # get items\n",
    "    selected_importances_mean[model_name] = importances_mean[model_name][maxk_indices]\n",
    "    selected_importances_std[model_name] = importances_std[model_name][maxk_indices]\n",
    "\n",
    "    # Get top names\n",
    "    selected_names[model_name] = np.array(feature_names)[maxk_indices]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea18110738c4527"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reset fonts\n",
    "font = {\n",
    "    \"family\": \"sans-serif\",\n",
    "    \"weight\": \"bold\",\n",
    "    \"size\": 12\n",
    "}\n",
    "matplotlib.rc(\"font\", **font)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(15, 60))\n",
    "axs = fig.subplots(8, 1)\n",
    "plot_model_names = list(importances_mean.keys())\n",
    "index = 0\n",
    "\n",
    "# Subplots\n",
    "for row in range(8):\n",
    "\n",
    "    axs[row].bar(selected_names[plot_model_names[index]], selected_importances_mean[plot_model_names[index]], width=0.5,  yerr=selected_importances_std[plot_model_names[index]])\n",
    "    axs[row].set_xticks(range(len(selected_names[plot_model_names[index]])))\n",
    "    axs[row].set_xticklabels(selected_names[plot_model_names[index]], rotation=45)\n",
    "\n",
    "    # Set labels etc\n",
    "    axs[row].set_title(f\"Feature Importance using permutation on {plot_model_names[index]}\")\n",
    "    axs[row].set_ylabel(\"Decrease in mean accuracy\")\n",
    "\n",
    "    index = index + 1\n",
    "\n",
    "# For some spacing\n",
    "fig.tight_layout(pad=4.0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c176a296b6c0d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reset fonts\n",
    "font = {\n",
    "    \"family\": \"sans-serif\",\n",
    "    \"weight\": \"bold\",\n",
    "    \"size\": 12\n",
    "}\n",
    "matplotlib.rc(\"font\", **font)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "axs = fig.subplots(2, 2)\n",
    "index = 0\n",
    "\n",
    "# Subplots\n",
    "# Only top four models\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        axs[row, col].bar(selected_names[top_f1score_models[index]], selected_importances_mean[top_f1score_models[index]], width=0.5,  yerr=selected_importances_std[top_f1score_models[index]])\n",
    "        axs[row, col].set_xticks(range(len(selected_names[top_f1score_models[index]])))\n",
    "        axs[row, col].set_xticklabels(selected_names[top_f1score_models[index]], rotation=45)\n",
    "    \n",
    "        # Set labels etc\n",
    "        axs[row, col].set_title(f\"Feature Importance using permutation on {top_f1score_models[index]}\")\n",
    "        axs[row, col].set_ylabel(\"Decrease in mean accuracy\")\n",
    "    \n",
    "        index = index + 1\n",
    "\n",
    "# For some spacing\n",
    "fig.tight_layout(pad=4.0)\n",
    "fig.savefig(os.path.join(fig_save_paths[\"save_paths\"][\"feature_imp\"], \"topkFeatures.png\"), dpi=600)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85c014917ee64316"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Identifying the top contributors\n",
    "\n",
    "- Which sensor is the most important?\n",
    "- Which type of sensor data was important?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cde6d2c03cc2525b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Update fonts\n",
    "font = {\n",
    "    \"family\": \"sans-serif\",\n",
    "    \"weight\": \"bold\",\n",
    "    \"size\": 16\n",
    "}\n",
    "matplotlib.rc(\"font\", **font)\n",
    "\n",
    "k = 50\n",
    "model_names = list(importances_mean.keys())\n",
    "# model_names = top_f1score_models\n",
    "sensor_location_features = []\n",
    "sensor_features = []\n",
    "extracted_features = []\n",
    "for model_name in model_names:\n",
    "\n",
    "    # Get the maximum-k for each model\n",
    "    maxk_influences_indices = np.argpartition(importances_mean[model_name], -k)[-k:]\n",
    "    # Get the features\n",
    "    maxk_feature_names = np.array(feature_names)[maxk_influences_indices]\n",
    "\n",
    "    # Split the maxk feature names\n",
    "    for feature_name in maxk_feature_names:\n",
    "        splits = feature_name.split(\"|\")\n",
    "        sensor_location_features.append(splits[0].strip())\n",
    "        sensor_features.append(splits[1].strip())\n",
    "        extracted_features.append(splits[2].strip())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1297e98bc38ee5f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the common contributors\n",
    "counted_sensor_location_features = Counter(sensor_location_features)\n",
    "counted_sensor_type_features = Counter(sensor_features)\n",
    "counted_extracted_features = Counter(extracted_features)\n",
    "\n",
    "# Normalize the data with the maximum\n",
    "max_val_1 = max(counted_sensor_location_features.values())\n",
    "max_val_2 = max(counted_sensor_type_features.values())\n",
    "max_val_3 = max(counted_extracted_features.values())\n",
    "for val in counted_sensor_location_features.keys():\n",
    "    counted_sensor_location_features[val] /= max_val_1\n",
    "for val in counted_sensor_type_features.keys():\n",
    "    counted_sensor_type_features[val] /= max_val_2\n",
    "for val in counted_extracted_features.keys():\n",
    "    counted_extracted_features[val] /= max_val_3\n",
    "\n",
    "\n",
    "# Update the sentinel names\n",
    "updated_counted_sensor_location_features = {}\n",
    "sentinels_modname = {\"DAQSentinel01\":\"L1\", \"DAQSentinel02\":\"L2\", \"DAQSentinel03\":\"L3\"}\n",
    "for sentinel in sentinels:\n",
    "    updated_counted_sensor_location_features[sentinels_modname[sentinel]] = counted_sensor_location_features[sentinel]\n",
    "    \n",
    "\n",
    "# histogram on the counts\n",
    "fig = plt.figure(figsize=(30, 8))\n",
    "axs = fig.subplots(1, 3)\n",
    "axs[0].bar(updated_counted_sensor_location_features.keys(), updated_counted_sensor_location_features.values(), width=0.5, color=\"darkgreen\")\n",
    "axs[1].bar(counted_sensor_type_features.keys(), counted_sensor_type_features.values(), width=0.5, color=\"darkgreen\")\n",
    "axs[2].bar(counted_extracted_features.keys(), counted_extracted_features.values(), width=0.5, color=\"darkgreen\")\n",
    "# axs[1].set_xticklabels(counted_extracted_features.keys(), rotation=90)\n",
    "axs[2].set_xticks(list(range(len(counted_extracted_features.keys()))))\n",
    "axs[2].set_xticklabels(counted_extracted_features.keys(), rotation=90)\n",
    "\n",
    "# Labels\n",
    "axs[0].set_xlabel(\"Sensor Location\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "axs[0].set_title(\"Feature Importance on sensor location\")\n",
    "\n",
    "axs[1].set_xlabel(\"Sensor Type\")\n",
    "axs[1].set_ylabel(\"Frequency\")\n",
    "axs[1].set_title(\"Feature Importance on sensor type\")\n",
    "\n",
    "axs[2].set_xlabel(\"Extracted Features\")\n",
    "axs[2].set_ylabel(\"Frequency\")\n",
    "axs[2].set_title(\"Feature Importance on extracted features\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(fig_save_paths[\"save_paths\"][\"feature_imp\"], \"top_contributors.png\"), dpi=600)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e6d6916727fc035"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important sensing parameter\n",
    "\n",
    "- For the three sensor locations, which sensor type is the most important"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df87ea4a45dbe6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 50\n",
    "# model_names = list(importances_mean.keys())\n",
    "model_names = top_f1score_models\n",
    "sensor_location_type = {}\n",
    "sensor_location_features = {}\n",
    "for model_name in model_names:\n",
    "\n",
    "    # Get the maximum-k for each model\n",
    "    maxk_influences_indices = np.argpartition(importances_mean[model_name], -k)[-k:]\n",
    "    # Get the features\n",
    "    maxk_feature_names = np.array(feature_names)[maxk_influences_indices]\n",
    "\n",
    "    # Split the maxk feature names\n",
    "    for feature_name in maxk_feature_names:\n",
    "        splits = feature_name.split(\"|\")\n",
    "        if splits[0] not in sensor_location_type.keys():\n",
    "            sensor_location_type[splits[0]] = []\n",
    "        else:\n",
    "            sensor_location_type[splits[0]].append(splits[1])\n",
    "            \n",
    "        if splits[0] not in sensor_location_features.keys():\n",
    "            sensor_location_features[splits[0]] = []\n",
    "        else:\n",
    "            sensor_location_features[splits[0]].append(splits[2])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "852523e5c7653ce2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# histogram on the counts\n",
    "fig = plt.figure(figsize=(30, 8))\n",
    "axs = fig.subplots(1, 3)\n",
    "sentinels_modname = [\"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "for index, ax in enumerate(axs):\n",
    "    counter_vals = Counter(sensor_location_type[sentinels[index]])\n",
    "    \n",
    "    # Normalize the vals\n",
    "    max_val = max(counter_vals.values())\n",
    "    for val in counter_vals.keys():\n",
    "        counter_vals[val] /= max_val\n",
    "    \n",
    "    ax.bar(counter_vals.keys(), counter_vals.values(), width=0.5, color=\"darkgreen\")\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Sensor Type\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"Location - {sentinels_modname[index]}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(fig_save_paths[\"save_paths\"][\"feature_imp\"], \"loc_top_sensor-type.png\"), dpi=600)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6eebd2ec68e8927"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# histogram on the counts\n",
    "fig = plt.figure(figsize=(30, 8))\n",
    "axs = fig.subplots(1, 3)\n",
    "\n",
    "for index, ax in enumerate(axs):\n",
    "    counter_vals = Counter(sensor_location_features[sentinels[index]])\n",
    "    \n",
    "    # Normalize the vals\n",
    "    max_val = max(counter_vals.values())\n",
    "    for val in counter_vals.keys():\n",
    "        counter_vals[val] /= max_val\n",
    "    \n",
    "    ax.bar(counter_vals.keys(), counter_vals.values(), width=0.5, color=\"darkgreen\")\n",
    "    ax.set_xticks(list(range(len(counted_extracted_features.keys()))))\n",
    "    ax.set_xticklabels(counted_extracted_features.keys(), rotation=90)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Sensor Type\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"Location - {sentinels_modname[index]}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(fig_save_paths[\"save_paths\"][\"feature_imp\"], \"loc_top_features.png\"), dpi=600)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "910f3e536506b85d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43ba9ff6866d4c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
